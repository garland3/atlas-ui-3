# Prefect K3s Job Runner Setup Guide

Last updated: 2026-02-24

## Overview

The K3s Job Runner MCP server allows Atlas users to execute Python code as isolated Kubernetes Jobs, orchestrated by [Prefect](https://www.prefect.io/). This provides a secure, sandboxed environment for running user-submitted code with full observability through the Prefect UI.

## Architecture

```
User chat -> MCP tool (run_python_job_tool)
                |
                v
         Prefect Server API (http://prefect-server.atlas:4200/api)
                |
                v
         Prefect Worker (polls work queue)
                |
                v
         K8s Job (python:3.12-slim container)
                |
                v
         MCP tool polls for completion, returns stdout/stderr
```

### Components

| Component | Image | Purpose |
|-----------|-------|---------|
| PostgreSQL 16 | `postgres:16-alpine` | Prefect state database |
| Prefect Server | `prefecthq/prefect:3-latest` | API + UI |
| Prefect Worker | `prefecthq/prefect:3-latest` | Polls queue, creates K8s Jobs |
| Flow Runner | `localhost/atlas-prefect-runner:latest` | Executes Python code in Jobs |

All components run in the `atlas` namespace alongside other Atlas services.

## Prerequisites

- K3s cluster with Atlas deployed (`deploy/k3s/run.sh up`)
- Podman for building container images
- Prefect flow runner image built and imported into K3s

## Deployment

### 1. Build images

```bash
bash deploy/k3s/run.sh build
```

This builds the `atlas-prefect-runner` image alongside the other Atlas images and imports them into K3s containerd.

### 2. Deploy the stack

```bash
bash deploy/k3s/run.sh up
```

This applies all Prefect manifests (30-33) in order:
1. PostgreSQL database
2. Prefect server
3. Prefect worker
4. Init job (creates the `kubernetes-pool` work pool)

### 3. Register the flow deployment

After the Prefect server is healthy, register the python-runner flow:

```bash
# Port-forward to the Prefect server
sudo k3s kubectl port-forward -n atlas svc/prefect-server 4200:4200 &

# Run the deploy script
cd deploy/prefect
PREFECT_API_URL=http://localhost:4200/api python deploy_flow.py
```

### 4. Verify

- Prefect UI: `http://<server>:8080/prefect`
- Check pods: `sudo k3s kubectl get pods -n atlas -l 'app in (prefect-server,prefect-worker,prefect-postgres)'`
- Check work pool: `sudo k3s kubectl logs -n atlas job/prefect-init`

## MCP Server Configuration

The `k3s_job_runner` MCP server is registered in `atlas/config/mcp.json`:

```json
{
  "k3s_job_runner": {
    "command": ["python", "mcp/k3s_job_runner/main.py"],
    "cwd": "atlas",
    "groups": ["admins"],
    "compliance_level": "Internal"
  }
}
```

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `PREFECT_API_URL` | `http://prefect-server.atlas:4200/api` | Prefect server API endpoint |

## Usage

In Atlas chat (as an admin user), ask the LLM to run Python code:

> "Run this Python script: `print('Hello from K8s!')`"

The LLM will invoke the `k3s_job_runner_run_python_job_tool` MCP tool, which:
1. Submits the code to Prefect as a flow run
2. The worker picks it up and creates a K8s Job
3. The Job runs the code in an isolated container
4. Results (stdout/stderr) are returned to the chat

### Tool Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `code` | string | (required) | Python 3.12 source code |
| `timeout_seconds` | int | 120 | Max execution time (10-600s) |

### Limits

- Code length: 50,000 characters max
- Memory: 256Mi per job
- CPU: 500m per job
- Output: 100,000 characters max (truncated)
- Timeout: 600 seconds max

## Security

### Container Isolation

Flow runner containers are locked down:
- Non-root user (UID 1000)
- Read-only root filesystem (writable `/tmp` only)
- No privilege escalation
- All Linux capabilities dropped
- `automountServiceAccountToken: false` (no K8s API access)
- Resource limits enforced

### Access Control

- MCP tool restricted to `admins` group via `mcp.json`
- Compliance level: `Internal`
- Prefect UI is behind auth middleware (Traefik auth-chain)

### Worker RBAC

The Prefect worker has namespace-scoped RBAC:
- Can create/list/delete Jobs in `atlas` namespace
- Can get/list Pods and read Pod logs
- No cluster-wide permissions

## Troubleshooting

### Prefect server not starting
```bash
# Check logs
sudo k3s kubectl logs -n atlas deployment/prefect-server

# Check PostgreSQL
sudo k3s kubectl logs -n atlas deployment/prefect-postgres
```

### Worker not picking up jobs
```bash
# Check worker logs
sudo k3s kubectl logs -n atlas deployment/prefect-worker

# Verify work pool exists
sudo k3s kubectl port-forward -n atlas svc/prefect-server 4200:4200
curl http://localhost:4200/api/work_pools/
```

### Init job failing
```bash
# Check init job logs
sudo k3s kubectl logs -n atlas job/prefect-init

# Re-run init job
sudo k3s kubectl delete job prefect-init -n atlas
sudo k3s kubectl apply -f deploy/k3s/33-prefect-init-job.yaml
```

### Flow not registered
```bash
# Re-run the deploy script
cd deploy/prefect
PREFECT_API_URL=http://localhost:4200/api python deploy_flow.py
```

## Monitoring

The Prefect UI at `/prefect` provides:
- Flow run history and status
- Log viewer for each run
- Work pool and queue monitoring
- Worker status

Access requires authentication through the Atlas auth middleware.
